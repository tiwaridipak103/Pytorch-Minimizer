# -*- coding: utf-8 -*-
"""torchtrustncg.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1X4KKoeJdBrKBhqPY0Ah-_v68wmB4RsSz
"""

!pip install lmfit
!pip install pytorch-minimize

!pip install loguru

from __future__ import absolute_import
from __future__ import print_function
from __future__ import division

from typing import NewType, List, Tuple

import torch
from torch import norm
import torch.optim as optim
import torch.autograd as autograd

from loguru import logger

import math

Tensor = NewType('Tensor', torch.Tensor)


def eye_like(tensor, device):
    return torch.eye(*tensor.size(), out=torch.empty_like(tensor, device=device), device=device)


class TrustRegion(optim.Optimizer):

    def __init__(
        self,
        params: List[Tensor],
        max_trust_radius: float = 1000,
        initial_trust_radius: float = 0.5,
        eta: float = 0.15,
        gtol: float = 1e-05,
        kappa_easy: float = 0.1,
        max_newton_iter: int = 50,
        max_krylov_dim: int = 15,
        lanczos_tol: float = 1e-4,
        opt_method: str = 'cg',
        epsilon: float = 1.0e-09,
        **kwargs
    ) -> None:
        """ Trust Region
                Newton Conjugate Gradient
                    Uses the Conjugate Gradient Algorithm to find the solution of the
                    trust region sub-problem. For more details see Algorithm 7.2 of
                    "Numerical Optimization, Nocedal and Wright"
                Generalized Lanczos Method
                    Uses the GEneralized Lanczos Algorithm to find the solution of the
                    trust region sub-problem. For more details see Algorithm7.5.2 of
                    "Trust Region Methods, Conn et al."
                    Arguments:
                        params (iterable): A list or iterable of tensors that will be
                            optimized
                        max_trust_radius: float
                            The maximum value for the trust radius
                        initial_trust_radius: float
                            The initial value for the trust region
                        eta: float
                            Minimum improvement ration for accepting a step
                        kappa_easy: float
                            Parameter related to the convergence of Krylov method, see Lemma 7.3.5 Conn et al.
                        max_newton_iter: int
                            Maximum Newton iterations for root finding
                        max_krylov_dim: int
                            Maximum Krylov dimension
                        lanczos_tol: float
                            Approximation error of the optimizer in Krylov subspace, see Theorem 7.5.10 Conn et al.
                        opt_method: string
                            The method to solve the subproblem.
                        gtol: float
                            Gradient tolerance for stopping the optimization
        """
        defaults = dict()

        super(TrustRegion, self).__init__(params, defaults)

        self.steps = 0
        self.max_trust_radius = max_trust_radius
        self.initial_trust_radius = initial_trust_radius
        self.eta = eta
        self.gtol = gtol
        self._params = self.param_groups[0]['params']

        self.kappa_easy = kappa_easy
        self.opt_method = opt_method
        self.lanczos_tol = lanczos_tol
        self.max_krylov_dim = max_krylov_dim
        self.max_newton_iter = max_newton_iter
        self.kwargs = kwargs

        self.epsilon = epsilon

        self.T_lambda = lambda _lambda, T_x, device: T_x.to(
            device) + _lambda * eye_like(T_x, device)
        self.lambda_const = lambda lambda_k: (
            1 + lambda_k) * torch.sqrt(torch.tensor(torch.finfo(torch.float32).eps))

        if not (opt_method == 'cg' or opt_method == 'krylov'):
            raise ValueError('opt_method should be "cg" or "krylov"')

    @torch.enable_grad()
    def _compute_hessian_vector_product(
            self,
            gradient: Tensor,
            p: Tensor) -> Tensor:

        hess_vp = autograd.grad(
            torch.sum(gradient * p, dim=-1), self._params,
            only_inputs=True, retain_graph=True, allow_unused=True)
        return torch.cat([torch.flatten(vp) for vp in hess_vp], dim=-1)
        #  hess_vp = torch.cat(
        #  [torch.flatten(vp) for vp in hess_vp], dim=-1)
        #  return torch.flatten(hess_vp)

    def _gather_flat_grad(self) -> Tensor:
        """ Concatenates all gradients into a single gradient vector
        """
        views = []
        for p in self._params:
            if p.grad is None:
                view = p.data.new(p.data.numel()).zero_()
            elif p.grad.data.is_sparse:
                view = p.grad.to_dense().view(-1)
            else:
                view = p.grad.view(-1)
            views.append(view)
        output = torch.cat(views, 0)
        return output

    @torch.no_grad()
    def _improvement_ratio(self, p, start_loss, gradient, closure):
        """ Calculates the ratio of the actual to the expected improvement
            Arguments:
                p (torch.tensor): The update vector for the parameters
                start_loss (torch.tensor): The value of the loss function
                    before applying the optimization step
                gradient (torch.tensor): The flattened gradient vector of the
                    parameters
                closure (callable): The function that evaluates the loss for
                    the current values of the parameters
            Returns:
                The ratio of the actual improvement of the loss to the expected
                improvement, as predicted by the local quadratic model
        """

        # Apply the update on the parameter to calculate the loss on the new
        # point
        hess_vp = self._compute_hessian_vector_product(gradient, p)

        # Apply the update of the parameter vectors.
        # Use a torch.no_grad() context since we are updating the parameters in
        # place
        with torch.no_grad():
            start_idx = 0
            for param in self._params:
                num_els = param.numel()
                curr_upd = p[start_idx:start_idx + num_els]
                param.data.add_(curr_upd.view_as(param))
                start_idx += num_els

        # No need to backpropagate since we only need the value of the loss at
        # the new point to find the ratio of the actual and the expected
        # improvement
        new_loss = closure(backward=False)
        # The numerator represents the actual loss decrease
        numerator = start_loss - new_loss

        new_quad_val = self._quad_model(p, start_loss, gradient, hess_vp)

        # The denominator
        denominator = start_loss - new_quad_val

        # TODO: Convert to epsilon, print warning
        ratio = numerator / (denominator + 1e-20)
        return ratio

    @torch.no_grad()
    def _quad_model(
            self,
            p: Tensor,
            loss: float,
            gradient: Tensor,
            hess_vp: Tensor) -> float:
        """ Returns the value of the local quadratic approximation
        """
        return (loss + torch.flatten(gradient * p).sum(dim=-1) +
                0.5 * torch.flatten(hess_vp * p).sum(dim=-1))

    @torch.no_grad()
    def calc_boundaries(
            self,
            iterate: Tensor,
            direction: Tensor,
            trust_radius: float) -> Tuple[Tensor, Tensor]:
        """ Calculates the offset to the boundaries of the trust region
        """

        a = torch.sum(direction ** 2)
        b = 2 * torch.sum(direction * iterate)
        c = torch.sum(iterate ** 2) - trust_radius ** 2
        sqrt_discriminant = torch.sqrt(b * b - 4 * a * c)
        ta = (-b + sqrt_discriminant) / (2 * a)
        tb = (-b - sqrt_discriminant) / (2 * a)
        if ta.item() < tb.item():
            return [ta, tb]
        else:
            return [tb, ta]

    @torch.no_grad()
    def _solve_subproblem_cg(
            self,
            loss: float,
            flat_grad: Tensor,
            trust_radius: float) -> Tuple[Tensor, bool]:
        ''' Solves the quadratic subproblem in the trust region
        '''

        # The iterate vector that contains the increment from the starting
        # point
        iterate = torch.zeros_like(flat_grad, requires_grad=False)

        # The residual of the CG algorithm
        residual = flat_grad.detach()
        # The first direction of descent
        direction = -residual

        jac_mag = torch.norm(flat_grad).item()
        # Tolerance define in Nocedal & Wright in chapter 7.1
        tolerance = min(0.5, math.sqrt(jac_mag)) * jac_mag

        # If the magnitude of the gradients is smaller than the tolerance then
        # exit
        if jac_mag <= tolerance:
            return iterate, False

        # Iterate to solve the subproblem
        while True:
            # Calculate the Hessian-Vector product
            #  start = time.time()
            hessian_vec_prod = self._compute_hessian_vector_product(
                flat_grad, direction
            )
            #  torch.cuda.synchronize()
            #  print('Hessian Vector Product', time.time() - start)

            # This term is equal to p^T * H * p
            #  start = time.time()
            hevp_dot_prod = torch.sum(hessian_vec_prod * direction)
            #  print('p^T H p', time.time() - start)

            # If non-positive curvature
            if hevp_dot_prod.item() <= 0:
                # Find boundaries and select minimum
                #  start = time.time()
                ta, tb = self.calc_boundaries(iterate, direction, trust_radius)
                pa = iterate + ta * direction
                pb = iterate + tb * direction

                # Calculate the point on the boundary with the smallest value
                bound1_val = self._quad_model(pa, loss, flat_grad,
                                              hessian_vec_prod)
                bound2_val = self._quad_model(pb, loss, flat_grad,
                                              hessian_vec_prod)
                #  torch.cuda.synchronize()
                #  print('First if', time.time() - start)
                #  print()
                if bound1_val.item() < bound2_val.item():
                    return pa, True
                else:
                    return pb, True

            # The squared euclidean norm of the residual needed for the CG
            # update
            #  start = time.time()
            residual_sq_norm = torch.sum(residual * residual, dim=-1)

            # Compute the step size for the CG algorithm
            cg_step_size = residual_sq_norm / hevp_dot_prod

            # Update the point
            next_iterate = iterate + cg_step_size * direction

            iterate_norm = torch.norm(next_iterate, dim=-1)
            #  torch.cuda.synchronize()
            #  print('CG Updates', time.time() - start)

            # If the point is outside of the trust region project it on the
            # border and return
            if iterate_norm.item() >= trust_radius:
                #  start = time.time()
                ta, tb = self.calc_boundaries(iterate, direction, trust_radius)
                p_boundary = iterate + tb * direction

                #  torch.cuda.synchronize()
                #  print('Second if', time.time() - start)
                #  print()
                return p_boundary, True

            #  start = time.time()
            # Update the residual
            next_residual = residual + cg_step_size * hessian_vec_prod
            #  torch.cuda.synchronize()
            #  print('Residual update', time.time() - start)
            # If the residual is small enough, exit
            if torch.norm(next_residual, dim=-1).item() < tolerance:
                #  print()
                return next_iterate, False

            #  start = time.time()
            beta = torch.sum(next_residual ** 2, dim=-1) / residual_sq_norm
            # Compute the new search direction
            direction = (-next_residual + beta * direction).squeeze()
            if torch.isnan(direction).sum() > 0:
                raise RuntimeError

            iterate = next_iterate
            residual = next_residual
            #  torch.cuda.synchronize()
            #  print('Replacing vectors', time.time() - start)
            #  print(trust_radius)
            #  print()

    @torch.no_grad()
    def _converged(self, s, trust_radius):

        if abs(norm(s) - trust_radius) <= self.kappa_easy * trust_radius:
            return True
        else:
            return False

    @torch.no_grad()
    def _lambda_one_plus(self, T, device):

        eigen_pairs = torch.linalg.eigh(T)

        Lambda, U = eigen_pairs.eigenvalues, eigen_pairs.eigenvectors
        lambda_n, u_n = Lambda[0].to(device=device), U[:, 0].to(device=device)

        return torch.maximum(-lambda_n, torch.tensor([0], device=device)), lambda_n, u_n[:, None]

    @torch.no_grad()
    def _quad_model_krylov(
            self,
            lanczos_g: Tensor,
            loss: float,
            s_x: Tensor,
            T_x: Tensor) -> float:
        """
         Returns the value of the local quadratic approximation
        """

        return (loss + torch.sum(lanczos_g * s_x) + 1 / 2 * torch.sum(T_x.mm(s_x) * s_x)).item()

    def _root_finder(self, trust_radius, T_x, lanczos_g, loss, device):

        n_iter_nu, n_iter_r = 0, 0
        lambda_k, lambda_n, u_n = self._lambda_one_plus(T_x, device)
        lambda_const = self.lambda_const(lambda_k).to(device=device)
        if lambda_k == 0:  # T_x is positive definite
            _lambda = torch.tensor(
                [0], dtype=torch.float32, device=device)  # + lambda_const
        else:
            _lambda = lambda_k + lambda_const

        s, L = self._compute_s(_lambda=_lambda, lambda_const=lambda_const,
                               lanczos_g=lanczos_g, T_x=T_x, device=device)

        if norm(s) <= trust_radius:

            if _lambda == 0 or norm(s) == trust_radius:
                return s
            else:
                ta, tb = self.calc_boundaries(
                    iterate=s, direction=u_n, trust_radius=trust_radius)
                pa = s + ta * u_n
                pb = s + tb * u_n

                # Calculate the point on the boundary with the smallest value
                bound1_val = self._quad_model_krylov(lanczos_g, loss, pa, T_x)
                bound2_val = self._quad_model_krylov(lanczos_g, loss, pb, T_x)

                if bound1_val < bound2_val:
                    return pa
                else:
                    return pb

        while True:
            if self._converged(s, trust_radius) or norm(s) < torch.finfo(float).eps:
                break

            # w = torch.triangular_solve(
            #     s, L.T.to(device=device), upper=False).solution
            w = torch.linalg.solve_triangular(
                L.T.to(device=device), s, upper=False)
            _lambda = self._nu_next(_lambda, trust_radius, s, w)

            s, L = self._compute_s(_lambda, lambda_const,
                                   lanczos_g, T_x, device)

            n_iter_nu += 1
            if n_iter_nu > self.max_newton_iter - 1:  # self.max_krylov_dim:
                print(RuntimeWarning(
                    'Maximum number of newton iterations exceeded for _lambda: {}'.format(_lambda)))
                break

        return s

    @torch.no_grad()
    def _nu_next(self, _lambda, trust_radius, s, w):

        norm_s = norm(s)
        norm_w = norm(w)

        phi = 1 / norm_s - 1 / trust_radius

        phi_prime = norm_w ** 2 / norm_s ** 3

        return _lambda - phi / phi_prime

    @torch.no_grad()
    def _compute_s(self, _lambda, lambda_const, lanczos_g, T_x, device):
        try:
            L = torch.linalg.cholesky(self.T_lambda(_lambda, T_x, device))
        except RuntimeError:
            # print('Recursion')
            lambda_const *= 2
            # RecursionError: maximum recursion depth exceeded while calling a Python object
            s, L = self._compute_s(
                _lambda + lambda_const, lambda_const, lanczos_g, T_x, device)

        s = torch.cholesky_solve(-lanczos_g[:, None],
                                 L.to(device=device), upper=True)
        return s, L

    @torch.no_grad()
    def _solve_subproblem_krylov(
            self,
            loss: float,
            flat_grad: Tensor,
            trust_radius: float) -> Tuple[Tensor, bool]:
        """
            Solves the quadratic subproblem in the trust region using Generalized Lanczos Method,
            see Algorithm 7.5.2 Conn et al.
        """
        INTERIOR_FLAG = True
        Q, diagonals, off_diagonals = [], [], []

        flat_grads_detached = flat_grad.detach()
        n_features = len(flat_grads_detached)
        h = torch.zeros_like(flat_grads_detached, requires_grad=False)
        q, p = flat_grads_detached, -flat_grads_detached

        gamma0 = torch.norm(q)

        krylov_dim, sigma = 0, 1

        device = flat_grad.device
        targs = {'device': device, 'dtype': flat_grad.dtype}

        while True:
            Hp = self._compute_hessian_vector_product(flat_grad, p)
            ptHp = torch.sum(Hp * p)
            alpha = torch.norm(q) ** 2 / ptHp
            # if alpha == 0:
            #     print('hard case')
            if krylov_dim == 0:
                diagonals.append(1. / alpha.clamp_(min=self.epsilon).item())
                off_diagonals.append(float('inf'))  # dummy value
                Q.append(sigma * q / norm(q))
                T_x = torch.tensor([diagonals], **targs)
                alpha_prev = alpha
            else:
                diagonals.append(1. / alpha.item() +
                                 beta.item() / alpha_prev.item())
                sigma = - torch.sign(alpha_prev) * sigma
                Q.append(sigma * q / norm(q))
                T_x = (torch.diag(torch.tensor(diagonals, **targs), 0)
                       + torch.diag(torch.tensor(off_diagonals[1:], **targs), -1)
                       + torch.diag(torch.tensor(off_diagonals[1:], **targs), 1))
                alpha_prev = alpha

            if INTERIOR_FLAG and alpha < 0 or torch.norm(h + alpha * p) >= trust_radius:
                INTERIOR_FLAG = False

            if INTERIOR_FLAG:
                h = h + alpha * p
            else:
                # Lanczos Step 2: solve problem in subspace
                e_1 = torch.eye(1, krylov_dim + 1,
                                device=flat_grad.device).flatten()
                lanczos_g = gamma0 * e_1
                s = self._root_finder(trust_radius=trust_radius,
                                      T_x=T_x, lanczos_g=lanczos_g,
                                      loss=loss, device=flat_grad.device)
                s = s.to(flat_grad.device)

            q_next = q + alpha * Hp

            # test for convergence
            if INTERIOR_FLAG and norm(q_next) ** 2 < self.lanczos_tol:
                break
            if not INTERIOR_FLAG and torch.norm(q_next) * abs(s[-1]) < self.lanczos_tol:
                break

            if krylov_dim == n_features:
                # print(RuntimeWarning(
                #     'Krylov dimensionality reach full space! Breaking out..'))
                break
                # return h

            if krylov_dim > self.max_krylov_dim:
                # print(RuntimeWarning('Max Krylov dimension reached! Breaking out..'))
                break

            beta = torch.dot(q_next, q_next) / torch.dot(q, q)
            off_diagonals.append(torch.sqrt(beta) / torch.abs(alpha_prev))
            p = -q_next + beta * p
            q = q_next
            krylov_dim = krylov_dim + 1

        if not INTERIOR_FLAG:
            # Return to the original space
            Q = torch.vstack(Q).T
            h = torch.sum(Q * torch.squeeze(s), dim=1)

        return h, not INTERIOR_FLAG  # INTERIOR_FLAG is False == hit_boundary is True

    def step(self, closure=None) -> float:
        starting_loss = closure(backward=True)

        flat_grad = self._gather_flat_grad()

        state = self.state
        if len(state) == 0:
            state['trust_radius'] = torch.full([1],
                                               self.initial_trust_radius,
                                               dtype=flat_grad.dtype,
                                               device=flat_grad.device)
        trust_radius = state['trust_radius']

        if self.opt_method == 'cg':
            param_step, hit_boundary = self._solve_subproblem_cg(
                starting_loss, flat_grad, trust_radius)
        else:
            param_step, hit_boundary = self._solve_subproblem_krylov(
                starting_loss, flat_grad, trust_radius)

        self.param_step = param_step

        if torch.norm(param_step).item() <= self.gtol:
            return starting_loss

        improvement_ratio = self._improvement_ratio(
            param_step, starting_loss, flat_grad, closure)

        if improvement_ratio.item() < 0.25:
            trust_radius.mul_(0.25)
        else:
            if improvement_ratio.item() > 0.75 and hit_boundary:
                trust_radius.mul_(2).clamp_(0.0, self.max_trust_radius)

        if improvement_ratio.item() <= self.eta:
            # If the improvement is not sufficient, then undo the update
            start_idx = 0
            for param in self._params:
                num_els = param.numel()
                curr_upd = param_step[start_idx:start_idx + num_els]
                param.data.add_(-curr_upd.view_as(param))
                start_idx += num_els

        self.steps += 1
        return starting_loss

from __future__ import absolute_import
from __future__ import print_function
from __future__ import division

import torch


def rosenbrock(tensor, alpha=1.0, beta=100):
    x, y = tensor[..., 0], tensor[..., 1]
    return (alpha - x) ** 2 + beta * (y - x ** 2) ** 2


def branin(tensor, **kwargs):
    x, y = tensor[..., 0], tensor[..., 1]
    loss = ((y - 0.129 * x ** 2 + 1.6 * x - 6) ** 2 + 6.07 * torch.cos(x) + 10)
    return loss


import numpy as np
import pandas as pd
from scipy.optimize import curve_fit, minimize
import collections
from lmfit import Parameters , minimize, Minimizer


name_1 = np.linspace(0.0, 5., 2000000)

name_2 = np.linspace(0.0, 10., 2000000)

name_3 = np.linspace(0.0, 15., 2000000)

name_4 = np.linspace(0.0, 20., 2000000)

name_5 = np.linspace(0.0, 25., 2000000)

name_6 = np.linspace(0.0, 3., 2000000)

name_7 = np.linspace(0.0, 6., 2000000)

name_8 = np.linspace(0.0, 9., 2000000)

name_9 = np.linspace(0.0, 5., 2000000)

name_11 = np.linspace(0.0, 2., 2000000)

name_12 = np.linspace(0.0, 4., 2000000)

Y_act  = np.linspace(0.0, 10., 2000000)
wgt  = np.linspace(0.0, 3., 2000000)

group = ['group_name' for i in range(2000000)]

df_data= pd.DataFrame({'name_1' : name_1 , 'name_2':name_2 , 'name_3':name_3 ,'name_4':name_4,'name_5':name_5,'name_6':name_6,
                       'name_7':name_7,'name_8':name_8,'name_9':name_9,'name_11':name_11,'name_12':name_12,
                       'Y_act':Y_act ,'wgt':wgt,'group':group })


name_10 = []
string = ['f_1', 'f_2', 'f_3' , 'f_4' ,'f_5']
for strs in string:
    if strs == 'f_1':
        name_10.extend(['f_1' for i in range(4)])
    elif strs == 'f_2':
        name_10.extend(['f_2' for i in range(4)])
    elif strs == 'f_3':
        name_10.extend(['f_3' for i in range(4)])
    elif strs == 'f_4':
        name_10.extend(['f_4' for i in range(4)])
    else:
        name_10.extend(['f_5' for i in range(4)])
print(len(name_10))

#################################################
import random
list1 = [i for i in range(1 , 3)]
list2 = [i for i in range(3 , 5)]
list3 = [i for i in range(2 , 4)]

name_A = [random.choice(list1) for i in range(20)]
name_B = [random.choice(list2) for i in range(20)]
name_C = [random.choice(list3) for i in range(20)]

#################################################

PARAM_1 = ['ABC_'+str(i) for i in range(20)]
value_1 = np.linspace(0.0, 25., 20)

PARAM_2 = ['XYZ_'+str(i) for i in range(20)]
value_2 = np.linspace(0.0, 50., 20)

#################################################
key = ['X'+str(i) for i in range(20)]

################################################
df_param= pd.DataFrame({'PARAM_1' : PARAM_1 ,'value_1' : value_1 , 'PARAM_2':PARAM_2 ,'value_2' : value_2 , 'name_10':name_10 ,
                        'name_A':name_A,'name_B':name_B,'name_C':name_C,'key':key})

def func_plin(f , x , a, b, c):
    if f == 'f_1':
        return np.maximum(0 , minimum(1 , (x-a)/(b-a)))
    elif f == 'f_2':
        return np.maximum(0 , minimum(1 , (x-a)/(b-c)))
    elif f == 'f_3':
        return np.maximum(0 , minimum(1 , (x-a)*(b-a)))
    elif f == 'f_4':
        return np.maximum(0 , minimum(1 , (x-a)*(b-c)))
    elif f == 'f_5':
        return np.maximum(0 , minimum(1 , (x-a)*(b-c)))
    else: 
        return (1)
 
##########################################################

def model_func_fit(pm_EY_EST , PM  ,funcid , submod, df):
    y_mod = model_func(pm_EY_EST , PM  ,funcid , submod, df)
    er = y_mod - df['Y_act']
    er = er * np.sqrt(np.abs(df['wgt']))
    return er

############################################################

def model_func(pm_EY_EST , PM ,funcid , submod , df):
    if submod == 'TOT':
        y_mod = model_func_tot(pm_EY_EST , PM , df ,funcid )
    elif submod == 'PRX':
        y_mod = model_func_prx(pm_EY_EST , PM , df ,funcid )
    else:
        y_mod = model_func_cst(pm_EY_EST , PM , df ,funcid )
    return  y_mod  
##############################################################

def model_func_pmilvl(pm_EY_EST , PM , df ,funcid):
    pm_2= PM.dict_1
    dict_F = PM.dict_2
    pm_AY_1 = PM.pm_AY
    #pm_EY_1 = pm_EY_EST + pm_AY_1
    pm_EY_1 = pm_AY_1 + pm_EY_EST
    
    pmilvl = 0 
    pmilvl = pmilvl + pm_EY_1['XYZ_0']+ (df['name_1']+ pm_EY_1['ABC_0']) +pm_EY_1['XYZ_5'] + pm_EY_1['ABC_5']
    pmilvl = pmilvl + pm_EY_1['XYZ_1']- (df['name_2']/ pm_EY_1['ABC_1']) +pm_EY_1['XYZ_6'] + pm_EY_1['ABC_6']
    pmilvl = pmilvl + pm_EY_1['XYZ_2']- (df['name_3']/ pm_EY_1['ABC_2']) +pm_EY_1['XYZ_7']+pm_EY_1['ABC_7']
    pmilvl = pmilvl + pm_EY_1['XYZ_3']- (df['name_4']/ pm_EY_1['ABC_3']) +pm_EY_1['XYZ_8']+ pm_EY_1['ABC_8']
    pmilvl = pmilvl + pm_EY_1['XYZ_4']- (df['name_5']/ pm_EY_1['ABC_4']) +pm_EY_1['XYZ_9']+ pm_EY_1['ABC_9']
    
    pmilvl_1 = 0
    if funcid == 1:
        for i in range(0 , 10):
            pmilvl_1 = pmilvl_1 + func_plin(pm_2['X'+str(i)] ,df['name_12']  ,dict_F['X'+str(i)][0], dict_F['X'+str(i)][1], dict_F['X'+str(i)][2])
        
        
    return  pmilvl +  pmilvl_1                                                                      
                                  
    
####################################################################    

def model_func_tot(pm_EY_EST , PM , df ,funcid):
    pmilvl = model_func_pmilvl(pm_EY_EST , PM , df ,funcid)
    
    pm_2= PM.dict_1
    dict_F = PM.dict_2
    pm_AY_1 = PM.pm_AY
    #pm_EY_1 = pm_EY_EST + pm_AY_1
    pm_EY_1 = pm_AY_1 + pm_EY_EST
    
    tot = 0 
    tot = tot + pm_EY_1['XYZ_10']- (df['name_6']+ pm_EY_1['ABC_15']) +pm_EY_1['ABC_10'] + pm_EY_1['XYZ_15']
    tot = tot + pm_EY_1['XYZ_11']- (df['name_7']/ pm_EY_1['ABC_16']) +pm_EY_1['ABC_11'] + pm_EY_1['XYZ_16']
    tot = tot + pm_EY_1['XYZ_12']- (df['name_8']/ pm_EY_1['ABC_17']) +pm_EY_1['ABC_12'] + pm_EY_1['XYZ_17']
    tot = tot + pm_EY_1['XYZ_13']- (df['name_9']/ pm_EY_1['ABC_18']) +pm_EY_1['ABC_13'] + pm_EY_1['XYZ_18']
    tot = tot + pm_EY_1['XYZ_14']- (df['name_11']/ pm_EY_1['ABC_19']) +pm_EY_1['ABC_14'] + pm_EY_1['XYZ_19']
    
    tot_1 = 0
    if funcid == 1:
        for i in range(0 , 10):
            tot_1 = tot_1 + func_plin(pm_2['X'+str(i)] ,df['name_12']  ,dict_F['X'+str(i)][0], dict_F['X'+str(i)][1], dict_F['X'+str(i)][2])
    
    elif  funcid == 2:   
        for i in range(10 , 20):
            tot_1 = tot_1 + func_plin(pm_2['X'+str(i)] ,df['name_12']  ,dict_F['X'+str(i)][0], dict_F['X'+str(i)][1], dict_F['X'+str(i)][2])
           
    return  pmilvl + tot +  np.exp(tot_1)    

########################################################

def model_func_prx(pm_EY_EST , PM , df ,funcid):
    pm_1= PM.dict_1
    dict_f = PM.dict_2
    pm_AY = PM.pm_AY
    pm_EY = pm_EY_EST + pm_AY
    
    prx = 0 
    prx = prx + pm_EY['XYZ_0']+ (df['name_1']+ pm_EY['ABC_0']) +pm_EY['XYZ_5'] + pm_EY['ABC_5']
    prx = prx + pm_EY['XYZ_1']- (df['name_2']/ pm_EY['ABC_1']) +pm_EY['XYZ_6'] + pm_EY['ABC_6']
    prx = prx + pm_EY['XYZ_2']- (df['name_3']/ pm_EY['ABC_2']) +pm_EY['XYZ_7']+ pm_EY['ABC_7']
    prx = prx + pm_EY['XYZ_3']- (df['name_4']/ pm_EY['ABC_3']) +pm_EY['XYZ_8']+ pm_EY['ABC_8']
    prx = prx + pm_EY['XYZ_4']- (df['name_5']/ pm_EY['ABC_4']) +pm_EY['XYZ_9']+ pm_EY['ABC_9']
    prx = prx + pm_EY['XYZ_10']- (df['name_6']/ pm_EY['ABC_15']) +pm_EY['ABC_10'] + pm_EY['XYZ_15']
    prx = prx + pm_EY['XYZ_11']- (df['name_7']/ pm_EY['ABC_16']) +pm_EY['ABC_11'] + pm_EY['XYZ_16']
    prx = prx + pm_EY['XYZ_12']- (df['name_8']/ pm_EY['ABC_17']) +pm_EY['ABC_12'] + pm_EY['XYZ_17']
    prx = prx + pm_EY['XYZ_13']- (df['name_9']/ pm_EY['ABC_18']) +pm_EY['ABC_13'] + pm_EY['XYZ_18']
    prx = prx + pm_EY['XYZ_14']- (df['name_11']/ pm_EY['ABC_19']) +pm_EY['ABC_14'] + pm_EY['XYZ_19']
    
    prx_1 = 0
    if funcid == 1:
        for i in range(0 , 10):
            prx_1 = prx_1 + func_plin(pm_1['X'+str(i)] ,df['name_12']  ,dict_f['X'+str(i)][0], dict_f['X'+str(i)][1], dict_f['X'+str(i)][2])
    
    elif  funcid == 2:   
        for i in range(10 , 20):
            prx_1 = prx_1 + func_plin(pm_1['X'+str(i)] ,df['name_12']  ,dict_f['X'+str(i)][0], dict_f['X'+str(i)][1], dict_f['X'+str(i)][2])
    
    return  np.exp(prx + prx_1) 

#######################################################################

def model_func_cst(pm_EY_EST , PM , df ,funcid):
    pm_1= PM.dict_1
    dict_f = PM.dict_2
    pm_AY = PM.pm_AY
    pm_EY = pm_EY_EST + pm_AY
    
    cst = 0 
    cst = cst + pm_EY['XYZ_0']+ (df['name_1']+ pm_EY['ABC_0']) /pm_EY['XYZ_5'] - pm_EY['ABC_5']
    cst = cst + pm_EY['XYZ_1']+ (df['name_2']/ pm_EY['ABC_1']) /pm_EY['XYZ_6'] - pm_EY['ABC_6']
    cst = cst + pm_EY['XYZ_2']+ (df['name_3']/ pm_EY['ABC_2']) /pm_EY['XYZ_7']- pm_EY['ABC_7']
    cst = cst + pm_EY['XYZ_3']+ (df['name_4']/ pm_EY['ABC_3']) /pm_EY['XYZ_8']- pm_EY['ABC_8']
    cst = cst + pm_EY['XYZ_4']+ (df['name_5']/ pm_EY['ABC_4']) /pm_EY['XYZ_9']- pm_EY['ABC_9']
    cst = cst + pm_EY['XYZ_10']+ (df['name_6']/ pm_EY_1['ABC_15']) /pm_EY['ABC_10'] + pm_EY['XYZ_15']
    cst = cst + pm_EY['XYZ_11']+ (df['name_7']/ pm_EY_1['ABC_16']) /pm_EY['ABC_11'] + pm_EY['XYZ_16']
    cst = cst + pm_EY['XYZ_12']+ (df['name_8']/ pm_EY_1['ABC_17']) /pm_EY['ABC_12'] + pm_EY['XYZ_17']
    cst = cst + pm_EY['XYZ_13']+ (df['name_9']/ pm_EY_1['ABC_18']) /pm_EY['ABC_13'] + pm_EY['XYZ_18']
    cst = cst + pm_EY['XYZ_14']+ (df['name_11']/ pm_EY_1['ABC_19']) /pm_EY['ABC_14'] + pm_EY['XYZ_19']
    
    cst_1 = 0
    if funcid == 1:
        for i in range(0 , 10):
            cst_1 = cst_1 + func_plin(pm_1['X'+str(i)] ,df['name_12']  ,dict_f['X'+str(i)][0], dict_f['X'+str(i)][1], dict_f['X'+str(i)][2])
    
    elif  funcid == 2:   
        for i in range(10 , 20):
            cst_1 = cst_1 + func_plin(pm_1['X'+str(i)] ,df['name_12']  ,dict_f['X'+str(i)][0], dict_f['X'+str(i)][1], dict_f['X'+str(i)][2])
    
    return  np.exp(cst + cst_1)

PM = model_param(df_param)



import datetime 
from numpy import *
start_time = datetime.datetime.now() 
print ("Time start time is  : ", end = "") 
print (start_time) 

#############################################

pm_EY_EST = PM.pm_EY
funcid = 1
submod = 'TOT'
out_batch_01 = Minimizer(model_func_fit, pm_EY_EST  ,fcn_args = ( PM  ,funcid , submod,df_data,))
out_batch_01 = out_batch_01.minimize()

########################################### 
end_time = datetime.datetime.now() 
print ("Time end time is  : ", end = "") 
print (end_time)

out_batch_01

def model_param(df):
    import collections
    dict_1 = {df['key'][i]:df['name_10'][i] for i in range(20)}
    dict_2 = {df['key'][i]:[df['name_A'][i],df['name_B'][i],df['name_C'][i]] for i in range(20)}
    dict_PM_1 = {df['PARAM_1'][i]:df['value_1'][i] for i in range(20)}
    dict_PM_2 = {df['PARAM_2'][i]:df['value_2'][i] for i in range(20)}
    dict_3 = {df['PARAM_1'][i]:df['key'][i] for i in range(20)}
    dict_4 = {df['PARAM_2'][i]:df['key'][i] for i in range(20)}
    
    pm_AY = Parameters()
    pm_EY = Parameters()
    for i in range(20):
        pm_AY.add('ABC_'+str(i), value =dict_PM_1['ABC_'+str(i)])
        pm_EY.add('XYZ_'+str(i), value =dict_PM_2['XYZ_'+str(i)])
        
    PM_col = collections.namedtuple('all_col' , ['dict_1','dict_2','pm_AY','pm_EY','dict_3','dict_4'])  
    PM = PM_col(dict_1=dict_1,dict_2=dict_2,pm_AY=pm_AY,pm_EY=pm_EY,dict_3=dict_3,dict_4=dict_4)
    
    return PM

import torch
from torch.functional import F

if torch.cuda.is_available():  
  dev = "cuda:0" 
else:  
  dev = "cpu"  



def func_plin(f , x , a, b, c):
    x = torch.tensor(x, dtype = torch.float32).to(dev)
    a = torch.tensor(a, dtype = torch.float32).to(dev)
    b = torch.tensor(b, dtype = torch.float32).to(dev)
    c = torch.tensor(c, dtype = torch.float32).to(dev)

    if f == 'f_1':
        return torch.maximum(torch.tensor(0, dtype = torch.float32).to(dev) , torch.minimum(torch.tensor(1, dtype = torch.float32).to(dev) , (x-a)/(b-a)))
    elif f == 'f_2':
        return torch.maximum(torch.tensor(0, dtype = torch.float32).to(dev) , torch.minimum(torch.tensor(1, dtype = torch.float32).to(dev) , (x-a)/(b-c)))
    elif f == 'f_3':
        return torch.maximum(torch.tensor(0, dtype = torch.float32).to(dev) , torch.minimum(torch.tensor(1, dtype = torch.float32).to(dev) , (x-a)*(b-a)))
    elif f == 'f_4':
        return torch.maximum(torch.tensor(0, dtype = torch.float32).to(dev) , torch.minimum(torch.tensor(1, dtype = torch.float32).to(dev) , (x-a)*(b-c)))
    elif f == 'f_5':
        return torch.maximum(torch.tensor(0, dtype = torch.float32).to(dev) , torch.minimum(torch.tensor(1, dtype = torch.float32).to(dev) , (x-a)*(b-c)))
    else: 
        return torch.tensor(1, dtype = torch.float32).to(dev)


def model_func_fit(values  ,funcid , submod, df):

    df_param_new = create_PM_dataframe(values)
    PM = model_param(df_param_new)
    pm_EY_EST = PM.pm_EY

    y_mod = model_func(pm_EY_EST , PM  ,funcid , submod, df)
 
    er = y_mod * torch.sqrt(torch.abs(torch.tensor(df['wgt'], dtype = torch.float32)))
    
  

    return er 



def model_func(pm_EY_EST , PM ,funcid , submod , df):
    if submod == 'TOT':
        y_mod = model_func_tot(pm_EY_EST , PM , df ,funcid )
    elif submod == 'PRX':
        y_mod = model_func_prx(pm_EY_EST , PM , df ,funcid )
    else:
        y_mod = model_func_cst(pm_EY_EST , PM , df ,funcid )
    return  y_mod  

def model_func_tot(pm_EY_EST , PM , df ,funcid):
    pmilvl = model_func_pmilvl(pm_EY_EST , PM , df ,funcid)
    
    pm_2= PM.dict_1
    dict_F = PM.dict_2
    pm_AY_1 = PM.pm_AY
    
    tot = torch.tensor(0, dtype = torch.float32) 
    tot = tot + pm_EY_EST['XYZ_10']- (torch.tensor(df['name_6'], dtype = torch.float32).to(dev)+ pm_AY_1['ABC_15']) +pm_AY_1['ABC_10'] + pm_EY_EST['XYZ_15']
    tot = tot + pm_EY_EST['XYZ_11']- (torch.tensor(df['name_7'], dtype = torch.float32).to(dev)/ pm_AY_1['ABC_16']) +pm_AY_1['ABC_11'] + pm_EY_EST['XYZ_16']
    tot = tot + pm_EY_EST['XYZ_12']- (torch.tensor(df['name_8'], dtype = torch.float32).to(dev)/ pm_AY_1['ABC_17']) +pm_AY_1['ABC_12'] + pm_EY_EST['XYZ_17']
    tot = tot + pm_EY_EST['XYZ_13']- (torch.tensor(df['name_9'], dtype = torch.float32).to(dev)/ pm_AY_1['ABC_18']) +pm_AY_1['ABC_13'] + pm_EY_EST['XYZ_18']
    tot = tot + pm_EY_EST['XYZ_14']- (torch.tensor(df['name_11'], dtype = torch.float32).to(dev)/ pm_AY_1['ABC_19']) +pm_AY_1['ABC_14'] + pm_EY_EST['XYZ_19']
    
    tot_1 = torch.tensor(0, dtype = torch.float32).to(dev)
    if funcid == 1:
        for i in range(0 , 10):
            tot_1 = tot_1 + func_plin(pm_2['X'+str(i)] ,torch.tensor(df['name_12'], dtype = torch.float32).to(dev)  ,dict_F['X'+str(i)][0], dict_F['X'+str(i)][1], dict_F['X'+str(i)][2])
    
    elif  funcid == 2:   
        for i in range(10 , 20):
            tot_1 = tot_1 + func_plin(pm_2['X'+str(i)] ,torch.tensor(df['name_12'], dtype = torch.float32).to(dev)  ,dict_F['X'+str(i)][0], dict_F['X'+str(i)][1], dict_F['X'+str(i)][2])
           
    return  pmilvl + tot +  torch.exp(tot_1)  



def model_func_pmilvl(pm_EY_EST , PM , df ,funcid):
    pm_2= PM.dict_1
    dict_F = PM.dict_2
    pm_AY_1 = PM.pm_AY
    
    pmilvl = torch.tensor(0, dtype = torch.float32).to(dev) 
    pmilvl = pmilvl + pm_EY_EST['XYZ_0']+ (torch.tensor(df['name_1'], dtype = torch.float32).to(dev)+ pm_AY_1['ABC_0']) +pm_EY_EST['XYZ_5'] + pm_AY_1['ABC_5']
    pmilvl = pmilvl + pm_EY_EST['XYZ_1']- (torch.tensor(df['name_2'], dtype = torch.float32).to(dev)/ pm_AY_1['ABC_1']) +pm_EY_EST['XYZ_6'] + pm_AY_1['ABC_6']
    pmilvl = pmilvl + pm_EY_EST['XYZ_2']- (torch.tensor(df['name_3'], dtype = torch.float32).to(dev)/ pm_AY_1['ABC_2']) +pm_EY_EST['XYZ_7']+pm_AY_1['ABC_7']
    pmilvl = pmilvl + pm_EY_EST['XYZ_3']- (torch.tensor(df['name_4'], dtype = torch.float32).to(dev)/ pm_AY_1['ABC_3']) +pm_EY_EST['XYZ_8']+ pm_AY_1['ABC_8']
    pmilvl = pmilvl + pm_EY_EST['XYZ_4']- (torch.tensor(df['name_5'], dtype = torch.float32).to(dev)/ pm_AY_1['ABC_4']) +pm_EY_EST['XYZ_9']+ pm_AY_1['ABC_9']
    
    pmilvl_1 = torch.tensor(0, dtype = torch.float32).to(dev)
    if funcid == 1:
        for i in range(0 , 10):
            pmilvl_1 = pmilvl_1 + func_plin(pm_2['X'+str(i)] ,torch.tensor(df['name_12'], dtype = torch.float32).to(dev)  ,dict_F['X'+str(i)][0], dict_F['X'+str(i)][1], dict_F['X'+str(i)][2])
        
        
    return  pmilvl +  pmilvl_1

def create_PM_dataframe(total_list, grade_optimizer):
  total_list = total_list    # .clone().detach().requires_grad_(False)
  name_10 = []
  string = ['f_1', 'f_2', 'f_3' , 'f_4' ,'f_5']
  for strs in string:
      if strs == 'f_1':
          name_10.extend(['f_1' for i in range(4)])
      elif strs == 'f_2':
          name_10.extend(['f_2' for i in range(4)])
      elif strs == 'f_3':
          name_10.extend(['f_3' for i in range(4)])
      elif strs == 'f_4':
          name_10.extend(['f_4' for i in range(4)])
      else:
          name_10.extend(['f_5' for i in range(4)])
  

  #################################################
  import random
  list1 = [i for i in range(1 , 3)]
  list2 = [i for i in range(3 , 5)]
  list3 = [i for i in range(2 , 4)]

  name_A = total_list[20:40]
  name_B = total_list[40:60]
  name_C = total_list[60:80]

  #################################################

  PARAM_1 = ['ABC_'+str(i) for i in range(20)]
  value_1 = total_list[:20]

  PARAM_2 = ['XYZ_'+str(i) for i in range(20)]
  value_2 = grade_optimizer

  #################################################
  key = ['X'+str(i) for i in range(20)]

  ################################################
  #df_param= pd.DataFrame({'PARAM_1' : PARAM_1 ,'value_1' : value_1 , 'PARAM_2':PARAM_2 ,'value_2' : value_2 , 'name_10':name_10 ,
  #                        'name_A':name_A,'name_B':name_B,'name_C':name_C,'key':key})
          

  df_param  = {'PARAM_1' : PARAM_1 ,'value_1' : value_1 , 'PARAM_2':PARAM_2 ,'value_2' : value_2 , 'name_10':name_10 ,
                          'name_A':name_A,'name_B':name_B,'name_C':name_C,'key':key}

  return df_param

# Commented out IPython magic to ensure Python compatibility.
import torch
from torch.functional import F
funcid = 1
submod = 'TOT'

values = []
values.extend(value_1)
values.extend(name_A)
values.extend(name_B)
values.extend(name_C)

values = torch.tensor(values, dtype = torch.float32).to(dev)

def model_func_fit(values  ,funcid , submod, df , grade_optimizer):

    df_param_new = create_PM_dataframe(values.to(dev) , grade_optimizer)
    PM = model_param(df_param_new)
    pm_EY_EST = PM.pm_EY

    y_mod = model_func(pm_EY_EST , PM  ,funcid , submod, df)
 
    er = y_mod * torch.sqrt(torch.abs(torch.tensor(df['wgt'], dtype = torch.float32).to(dev)))
    
  

    return er  

target = torch.tensor(df_data['Y_act'], dtype = torch.float32).to(dev) * torch.sqrt(torch.abs(torch.tensor(df_data['wgt'], dtype = torch.float32).to(dev)))

weight = torch.tensor(value_2, requires_grad=True, dtype = torch.float32)

optimizer = torch.optim.LBFGS([weight], max_iter=1000, lr=0.1)
guesses = []
losses = []

funcid = 1
submod = 'TOT'

def closure():
    optimizer.zero_grad()
    #output = model_func_fit(values.type(torch.DoubleTensor)  ,funcid , submod, df_data, weight.type(torch.DoubleTensor))
    #loss = F.mse_loss(output.type(torch.DoubleTensor), target.type(torch.DoubleTensor))
    output = model_func_fit(values ,funcid , submod, df_data, weight)
    loss = F.mse_loss(output.type(torch.DoubleTensor), target.type(torch.DoubleTensor))
    loss.backward()
    guesses.append(weight.clone())
    losses.append(loss.clone())
    return loss

# %time optimizer.step(closure) 
print(f"Minimum: {[v.item() for v in weight]}")
print(f"Number of steps: {len(guesses)}")

import torch

from torch.autograd import Variable

import matplotlib.pyplot as plt

import torchvision.datasets as dsets
import torchvision.transforms as transforms

def acc(model, data_loader, device):
    with torch.no_grad():
        correct = 0
        total = 0
        for samples, labels in data_loader:
            #######################
            #  USE GPU FOR MODEL  #
            #######################
            samples = Variable(samples.view(-1, 28 * 28)).to(device)
            labels = labels.to(dtype=torch.float32).to(device)
            outputs = model(samples)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            #######################
            #  USE GPU FOR MODEL  #
            #######################
            # Total correct predictions
            if torch.cuda.is_available():
                correct += (predicted.cpu() == labels.cpu()).sum()
            else:
                correct += (predicted == labels).sum()

        accuracy = 100 * correct / total
        return

import torch
from torch.functional import F
funcid = 1
submod = 'TOT'

values = []
values.extend(value_1)
values.extend(name_A)
values.extend(name_B)
values.extend(name_C)

values = torch.tensor(values, dtype = torch.float32).to(dev)

def model_func_fit(values  ,funcid , submod, df , grade_optimizer):

    df_param_new = create_PM_dataframe(values.to(dev) , grade_optimizer)
    PM = model_param(df_param_new)
    pm_EY_EST = PM.pm_EY

    y_mod = model_func(pm_EY_EST , PM  ,funcid , submod, df)
 
    er = y_mod * torch.sqrt(torch.abs(torch.tensor(df['wgt'], dtype = torch.float32).to(dev)))
    
  

    return er  

target = torch.tensor(df_data['Y_act'], dtype = torch.float32).to(dev) * torch.sqrt(torch.abs(torch.tensor(df_data['wgt'], dtype = torch.float32).to(dev)))

weight = torch.tensor(value_2, requires_grad=True, dtype = torch.float32)


guesses = []
losses = []

funcid = 1
submod = 'TOT'

opt_method = 'krylov'
optimizer = TrustRegion(
    [weight], max_trust_radius=1000, initial_trust_radius=.005,
    eta=0.15, kappa_easy=0.01, max_newton_iter=150, max_krylov_dim=150,
    lanczos_tol=1e-5, gtol=1e-05, hutchinson_approx=True,
    opt_method=opt_method)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

opt_method = 'krylov'
optimizer = TrustRegion(
[weight], max_trust_radius=1000, initial_trust_radius=.005,
eta=0.15, kappa_easy=0.01, max_newton_iter=150, max_krylov_dim=150,
lanczos_tol=1e-5, gtol=1e-05, hutchinson_approx=True,
opt_method=opt_method)

tr_losses = []
tr_accuracies, tst_accuracies = [], []
n_iter = 0
best_acc = 0.0
n_runs = 1  # For mean efficiency

running_loss = 0
running_samples = 0

def closure(backward=True):
  if torch.is_grad_enabled() and backward:
      optimizer.zero_grad()
  #model_outputs = model(samples)
  output = model_func_fit(values ,funcid , submod, df_data, weight)
  loss = F.mse_loss(output.type(torch.DoubleTensor), target.type(torch.DoubleTensor))
  #cri_loss = criterion(model_outputs, labels)
  if loss.requires_grad and backward:
      loss.backward(retain_graph=True, create_graph=True)
  return loss

tr_loss = optimizer.step(closure=closure)

print(f"Minimum: {[v.item() for v in weight]}")

tr_loss

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
device = "cpu"
for n_epoch in range(int(1000)):
  def closure(backward=True):
    if torch.is_grad_enabled() and backward:
      optimizer.zero_grad()
    #model_outputs = model(samples)
    output = model_func_fit(values ,funcid , submod, df_data, weight)
    loss = F.mse_loss(output.type(torch.DoubleTensor), target.type(torch.DoubleTensor))
    #cri_loss = criterion(model_outputs, labels)
    if loss.requires_grad and backward:
        loss.backward(retain_graph=True, create_graph=True)
    return loss

  tr_loss = optimizer.step(closure=closure)

  if n_epoch % 100 == 0:
    print(f"Minimum: {[v.item() for v in weight]}")
    print(f"tr_loss is {tr_loss}")

